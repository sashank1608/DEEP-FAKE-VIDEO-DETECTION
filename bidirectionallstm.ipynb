{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bidirectionallstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UukP5U_rOlVB",
        "outputId": "3e0236e1-729a-4330-b41f-23c42802396d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oDGcZ7COrnR",
        "outputId": "83510c5f-2f6a-4e45-ffab-3a0fd9f17aaa"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC2gIUpOOrsZ",
        "outputId": "ac4bf886-7d53-41b2-fc4b-5112669ebb60"
      },
      "source": [
        "        basedir = \"./LRCN_dataset1\"\n",
        "        print(os.path.isdir(basedir))\n",
        "        all_metadata_filenames = []\n",
        "        all_video_filenames = []\n",
        "        for dirpath, _, filenames in os.walk(basedir):\n",
        "            for filename in filenames:\n",
        "                if filename.endswith(\".json\"):\n",
        "                    all_metadata_filenames.append(os.path.join(dirpath, filename).replace(\"\\\\\",\"/\"))\n",
        "                elif filename.endswith(\".jpg\"):\n",
        "                    all_video_filenames.append(os.path.join(dirpath, filename).replace(\"\\\\\",\"/\"))      "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82hS9uAzOrvV"
      },
      "source": [
        "        metadata = {}\n",
        "        for cur_metadata_filename in all_metadata_filenames:\n",
        "            with open(cur_metadata_filename) as f_in:\n",
        "                cur_metadata = json.load(f_in)\n",
        "                for cur_key in cur_metadata:\n",
        "                    # Store expanded versions of paths of videos in metadata\n",
        "                    full_filename = os.path.join(os.path.dirname(cur_metadata_filename), cur_key.replace('.mp4','.jpg')).replace(\"\\\\\",\"/\")\n",
        "                    metadata[full_filename] = {'label': cur_metadata[cur_key]['label']}\n",
        "                    if 'original' in cur_metadata[cur_key] and cur_metadata[cur_key]['original']:\n",
        "                        full_orig_filename = os.path.join(os.path.dirname(cur_metadata_filename), cur_metadata[cur_key]['original'].replace('.mp4','.jpg')).replace(\"\\\\\",\"/\")\n",
        "                        metadata[full_filename]['original'] = full_orig_filename"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X75OXXKXOryG"
      },
      "source": [
        "        real_videos = []\n",
        "        fake_videos = []\n",
        "        for cur_video_filename in all_video_filenames:\n",
        "            if metadata[cur_video_filename]['label'] == \"FAKE\":\n",
        "                fake_videos.append(cur_video_filename)\n",
        "            elif metadata[cur_video_filename]['label'] == \"REAL\":\n",
        "                real_videos.append(cur_video_filename)\n",
        "        # Sort the pathnames since consecutive runs of os.walk aren't guaranteed\n",
        "        # to return all the data in the same order\n",
        "        real_videos.sort()\n",
        "        fake_videos.sort()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIhvydPnVaXA"
      },
      "source": [
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "paths,Y=[],[]\n",
        "fake=random.sample(fake_videos,len(real_videos))\n",
        "#real_videos=real_videos[:1200]\n",
        "#fake=fake[:2400]\n",
        "for x in real_videos:\n",
        "    paths.append(x)\n",
        "    Y.append(0)\n",
        "for x in fake:\n",
        "    paths.append(x)\n",
        "    Y.append(1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4oq6C5sOr06",
        "outputId": "44c87859-b384-48b9-ab6d-5d1d32287a6e"
      },
      "source": [
        "print(len(fake))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXaUO4LNOr4B"
      },
      "source": [
        "def shuffle(X,y):\n",
        "    new_train=[]\n",
        "    for m,n in zip(X,y):\n",
        "        new_train.append([m,n])\n",
        "    random.shuffle(new_train)\n",
        "    X,y=[],[]\n",
        "    for x in new_train:\n",
        "        X.append(x[0])\n",
        "        y.append(x[1])\n",
        "    return X,y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehRMrzy3Or66"
      },
      "source": [
        "paths,y=shuffle(paths,Y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAgDEyBxOr-A"
      },
      "source": [
        "def get_birghtness(img):\n",
        "    return img/img.max()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFu1wuVeOsA9"
      },
      "source": [
        "from PIL import Image \n",
        "def process_img(img):\n",
        "    imgs=[]\n",
        "    \n",
        "    for x in range(10):\n",
        "        imgs.append(get_birghtness(img[:,x*299:(x+1)*299,:]))\n",
        "    return np.array(imgs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLO97MaROsDl"
      },
      "source": [
        "\n",
        "def gets(paths):\n",
        "    al=[]\n",
        "    for x in paths:\n",
        "        img=cv2.cvtColor(cv2.imread(x),cv2.COLOR_BGR2RGB)\n",
        "        img=cv2.resize(img,(2990,299))\n",
        "        al.append(process_img(img))\n",
        "    return al\n",
        "def generator(paths,y,batch_size=16):\n",
        "    \n",
        "    while True:\n",
        "        for x in range(len(paths)//batch_size):\n",
        "            if x*batch_size+batch_size>len(paths):\n",
        "                yield np.array(gets(paths[x*batch_size:])),np.array(y[x*batch_size:])\n",
        "            yield np.array(gets(paths[x*batch_size:x*batch_size+batch_size])),np.array(y[x*batch_size:x*batch_size+batch_size])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS4iXq9COsG3"
      },
      "source": [
        "from keras.layers import Activation\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.core import Dense, Dropout, Flatten\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D, ZeroPadding3D\n",
        "from keras.layers import Input\n",
        "\n",
        "# from schedules import onetenth_4_8_12\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from random import shuffle\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmONruLoOsKA"
      },
      "source": [
        "def cnn_model(model_name, img_size, weights):\n",
        "    \"\"\"\n",
        "    Model definition using Xception net architecture\n",
        "    \"\"\"\n",
        "    input_size = (img_size, img_size, 3)\n",
        "    if model_name == \"xception\":\n",
        "        baseModel = Xception(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(img_size, img_size, 3)\n",
        "        )\n",
        "    elif model_name == \"iv3\":\n",
        "        baseModel = InceptionV3(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(img_size, img_size, 3)\n",
        "        )\n",
        "    elif model_name == \"irv2\":\n",
        "        baseModel = InceptionResNetV2(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(img_size, img_size, 3)\n",
        "        )\n",
        "    elif model_name == \"resnet\":\n",
        "        baseModel = ResNet50(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(img_size, img_size, 3)\n",
        "        )\n",
        "    elif model_name == \"nasnet\":\n",
        "        baseModel = NASNetLarge(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(img_size, img_size, 3)\n",
        "        )\n",
        "    elif model_name == \"ef0\":\n",
        "        baseModel = EfficientNetB0(\n",
        "            input_size,\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False\n",
        "        )\n",
        "    elif model_name == \"ef5\":\n",
        "        baseModel = EfficientNetB5(\n",
        "            input_size,\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False\n",
        "        )\n",
        "\n",
        "    headModel = baseModel.output\n",
        "    headModel = GlobalAveragePooling2D()(headModel)\n",
        "    headModel = Dense(\n",
        "        512,\n",
        "        activation=\"relu\",\n",
        "        kernel_initializer=\"he_uniform\",\n",
        "        name=\"fc1\")(\n",
        "        headModel\n",
        "    )\n",
        "    headModel = Dropout(0.4)(headModel)\n",
        "    predictions = Dense(\n",
        "        2,\n",
        "        activation=\"softmax\",\n",
        "        kernel_initializer=\"he_uniform\")(\n",
        "        headModel\n",
        "    )\n",
        "    model = Model(inputs=baseModel.input, outputs=predictions)\n",
        "\n",
        "    model.load_weights(weights)\n",
        "    print(\"Weights loaded...\")\n",
        "    model_lstm = Model(\n",
        "        inputs=baseModel.input,\n",
        "        outputs=model.get_layer(\"fc1\").output\n",
        "    )\n",
        "\n",
        "    for layer in baseModel.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    optimizer = Nadam(\n",
        "        lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004\n",
        "    )\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model_lstm\n",
        "\n",
        "\n",
        "def lstm_model(shape):\n",
        "    # Model definition\n",
        "    main_input = Input(\n",
        "        shape=(shape[0],\n",
        "               shape[1]),\n",
        "        name=\"main_input\"\n",
        "    )\n",
        "    headModel = Bidirectional(LSTM(256, return_sequences=True))(main_input)\n",
        "    #headModel = LSTM(32)(main_input)\n",
        "    headModel = TemporalMaxPooling()(headModel)\n",
        "    # headModel = TimeDistributed(Dense(512))(headModel)\n",
        "    # # headModel = Bidirectional(LSTM(512, dropout=0.2))(main_input)\n",
        "    # headModel = LSTM(256)(headModel)\n",
        "    predictions = Dense(1,activation='sigmoid')(headModel)\n",
        "    model = Model(inputs=main_input, outputs=predictions)\n",
        "\n",
        "    # Model compilation\n",
        "    # opt = SGD(lr=1e-4, momentum=0.9, decay=1e-4 / EPOCHS)\n",
        "    optimizer = Nadam(\n",
        "        lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004\n",
        "    )\n",
        "    \n",
        "\n",
        "    return model\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.engine import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "import numpy as np\n",
        "from keras import utils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "class TemporalMaxPooling(Layer):\n",
        "    \"\"\"\n",
        "    This pooling layer accepts the temporal sequence output by a recurrent layer\n",
        "    and performs temporal pooling, looking at only the non-masked portion of the sequence.\n",
        "    The pooling layer converts the entire variable-length hidden vector sequence\n",
        "    into a single hidden vector.\n",
        "    Modified from https://github.com/fchollet/keras/issues/2151 so code also\n",
        "    works on tensorflow backend. Updated syntax to match Keras 2.0 spec.\n",
        "    Args:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "        input shape: (nb_samples, nb_timesteps, nb_features)\n",
        "        output shape: (nb_samples, nb_features)\n",
        "    Examples:\n",
        "        > x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
        "        > x = TemporalMaxPooling()(x)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TemporalMaxPooling, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.input_spec = InputSpec(ndim=3)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2])\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if mask is None:\n",
        "            mask = K.sum(K.ones_like(x), axis=-1)\n",
        "\n",
        "        # if masked, set to large negative value so we ignore it\n",
        "        # when taking max of the sequence\n",
        "        # K.switch with tensorflow backend is less useful than Theano's\n",
        "        if K.backend() != \"theano\":\n",
        "            mask = K.expand_dims(mask, axis=-1)\n",
        "            mask = K.tile(mask, (1, 1, K.int_shape(x)[2]))\n",
        "            masked_data = tf.where(\n",
        "                K.equal(mask, K.zeros_like(mask)), K.ones_like(x) * -np.inf, x\n",
        "            )  # if masked assume value is -inf\n",
        "            return K.max(masked_data, axis=1)\n",
        "        else:  # theano backend\n",
        "            mask = mask.dimshuffle(0, 1, \"x\")\n",
        "            masked_data = K.switch(K.eq(mask, 0), -np.inf, x)\n",
        "            return masked_data.max(axis=1)\n",
        "\n",
        "    def compute_mask(self, input, mask):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exvNsOWEpAWt",
        "outputId": "1a108ea3-39d6-4078-c4b4-8d2a0f72f4a7"
      },
      "source": [
        "!pip install Xception"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement Xception (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for Xception\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmSrJ_VysSuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd91886-7a17-4b68-9e14-0f25f48f2766"
      },
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.applications.xception import decode_predictions\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "xcep=tf.keras.applications.xception.Xception(weights='imagenet',include_top=True)\n",
        "from keras.layers import *\n",
        "inp=Input((10,299,299,3))\n",
        "x=TimeDistributed(xcep)(inp)\n",
        "x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
        "x = TemporalMaxPooling()(x)\n",
        "x = Dense(1,activation='sigmoid')(x)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels.h5\n",
            "91889664/91884032 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot4qTd7wQ3Rv"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def precision_fake(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives +K.epsilon())\n",
        "        return precision\n",
        "def recall_fake(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(1.0*y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives +K.epsilon())\n",
        "        return recall   \n",
        "def precision_real(y_true, y_pred):\n",
        "        true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "        predicted_negatives = K.sum(K.round(K.clip((1-y_pred), 0, 1)))\n",
        "        precision = true_negatives / (predicted_negatives+K.epsilon() )\n",
        "        return precision\n",
        "def recall_real(y_true, y_pred):\n",
        "        true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "        possible_negatives = K.sum(K.round(K.clip(1.0-y_true, 0, 1)))\n",
        "        recall = true_negatives / (possible_negatives + K.epsilon())\n",
        "        return recall                \n",
        "def F1_fake(y_true, y_pred):\n",
        "  precision = precision_fake(y_true, y_pred)\n",
        "  recall = recall_fake(y_true, y_pred)\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))   \n",
        "def F1_real(y_true, y_pred):\n",
        "  precision = precision_real(y_true, y_pred)\n",
        "  recall = recall_real(y_true, y_pred)\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))   \n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgUlbk5HQ36L"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "X_train, X_test, y_train, y_test=train_test_split(paths,y,test_size=0.25)\n",
        "from keras import Model\n",
        "model=Model(inp,x)\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def schedule(epoch):\n",
        "    return [6e-4,1e-4][epoch]\n",
        "callback=LearningRateScheduler(schedule)\n",
        "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4),metrics=['accuracy',f1,precision_real,precision_fake,recall_real,recall_fake,F1_real,F1_fake])\n",
        "#model.fit(X,y,batch_size=16)\n",
        "filepath = \"./bidirectional_lstm.h5\"\n",
        "checkpoint=ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "stopping = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=0)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnI1vWPnQ39B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh9VRGIvQ3yB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYrNKTzQOsNF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LyLx9yBOsP2",
        "outputId": "338cb77f-e911-4483-d8f5-e3c24c17b783"
      },
      "source": [
        "history = model.fit(generator(X_train,y_train,4),steps_per_epoch=len(X_train)//4+1,validation_data=generator(X_test,y_test,4),validation_steps=len(X_test)//4+1,epochs=18,callbacks=[checkpoint, stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/18\n",
            "  2/727 [..............................] - ETA: 9:35 - loss: 0.6931 - accuracy: 0.5000 - f1: 0.6000 - precision_real: 0.5000 - precision_fake: 0.8333 - recall_real: 0.2500 - recall_fake: 0.6250 - F1_real: 0.3333 - F1_fake: 0.6000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.6306s vs `on_train_batch_end` time: 0.9568s). Check your callbacks.\n",
            "727/727 [==============================] - ETA: 0s - loss: 0.5907 - accuracy: 0.6912 - f1: 0.6170 - precision_real: 0.6745 - precision_fake: 0.7007 - recall_real: 0.7456 - recall_fake: 0.6162 - F1_real: 0.6723 - F1_fake: 0.6170\n",
            "Epoch 00001: loss improved from inf to 0.59074, saving model to ./bidirectional_lstm.h5\n",
            "727/727 [==============================] - 1613s 2s/step - loss: 0.5907 - accuracy: 0.6912 - f1: 0.6170 - precision_real: 0.6745 - precision_fake: 0.7007 - recall_real: 0.7456 - recall_fake: 0.6162 - F1_real: 0.6723 - F1_fake: 0.6170 - val_loss: 0.5982 - val_accuracy: 0.7263 - val_f1: 0.7168 - val_precision_real: 0.6608 - val_precision_fake: 0.6581 - val_recall_real: 0.5103 - val_recall_fake: 0.8663 - val_F1_real: 0.5496 - val_F1_fake: 0.7168\n",
            "Epoch 2/18\n",
            "727/727 [==============================] - ETA: 0s - loss: 0.4338 - accuracy: 0.8071 - f1: 0.7623 - precision_real: 0.7984 - precision_fake: 0.8065 - recall_real: 0.8007 - recall_fake: 0.7823 - F1_real: 0.7705 - F1_fake: 0.7623\n",
            "Epoch 00002: loss improved from 0.59074 to 0.43375, saving model to ./bidirectional_lstm.h5\n",
            "727/727 [==============================] - 1360s 2s/step - loss: 0.4338 - accuracy: 0.8071 - f1: 0.7623 - precision_real: 0.7984 - precision_fake: 0.8065 - recall_real: 0.8007 - recall_fake: 0.7823 - F1_real: 0.7705 - F1_fake: 0.7623 - val_loss: 0.4684 - val_accuracy: 0.8220 - val_f1: 0.7836 - val_precision_real: 0.7819 - val_precision_fake: 0.7514 - val_recall_real: 0.6752 - val_recall_fake: 0.8741 - val_F1_real: 0.7006 - val_F1_fake: 0.7836\n",
            "Epoch 3/18\n",
            "727/727 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.8624 - f1: 0.8213 - precision_real: 0.8435 - precision_fake: 0.8463 - recall_real: 0.8337 - recall_fake: 0.8435 - F1_real: 0.8159 - F1_fake: 0.8213\n",
            "Epoch 00003: loss improved from 0.43375 to 0.32989, saving model to ./bidirectional_lstm.h5\n",
            "727/727 [==============================] - 1362s 2s/step - loss: 0.3299 - accuracy: 0.8624 - f1: 0.8213 - precision_real: 0.8435 - precision_fake: 0.8463 - recall_real: 0.8337 - recall_fake: 0.8435 - F1_real: 0.8159 - F1_fake: 0.8213 - val_loss: 0.4333 - val_accuracy: 0.8457 - val_f1: 0.7717 - val_precision_real: 0.8141 - val_precision_fake: 0.7932 - val_recall_real: 0.8045 - val_recall_fake: 0.7987 - val_F1_real: 0.7830 - val_F1_fake: 0.7717\n",
            "Epoch 4/18\n",
            "534/727 [=====================>........] - ETA: 5:38 - loss: 0.2679 - accuracy: 0.8904 - f1: 0.8567 - precision_real: 0.8641 - precision_fake: 0.8750 - recall_real: 0.8563 - recall_fake: 0.8761 - F1_real: 0.8398 - F1_fake: 0.8567"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ot4lnlDRV9s"
      },
      "source": [
        "with open('/trainHistoryDict', 'wb') as file_pi:\n",
        "        pickle.dump(history.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "AbTiVtIJRV6i",
        "outputId": "0aea321c-9642-4ccc-d7bb-23588f83fd90"
      },
      "source": [
        "print(history.history.keys())\n",
        "#  \"Accuracy\"\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-c8be0560ffe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#  \"Accuracy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    }
  ]
}